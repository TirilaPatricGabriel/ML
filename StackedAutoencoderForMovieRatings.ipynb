{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs2+dGPkymGUh3T2hmeDhU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TirilaPatricGabriel/ML/blob/main/StackedAutoencoderForMovieRatings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "cjKgo5XxpCZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "-Scx5TgmmJTR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.parallel\n",
        "import torch.utils.data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "TICXn1Qpo-IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# userId - gender - age - userJob - zip code\n",
        "users = pd.read_csv('users.dat', header=None, sep='::', engine='python', encoding='latin-1')\n",
        "\n",
        "# userId - movieId - grade\n",
        "ratings = pd.read_csv('ratings.dat', header=None, sep='::', engine='python', encoding='latin-1')\n",
        "\n",
        "# movieId - title - genre\n",
        "movies = pd.read_csv('movies.dat', header=None, sep='::', engine='python', encoding='latin-1')\n",
        "\n",
        "print(users.values[:2, :])\n",
        "print(ratings.values[:2, :])\n",
        "print(movies.values[:2, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAa5C87Ro7yB",
        "outputId": "be5fbd76-0e4f-4927-bba7-3dcf80e306d2"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 'F' 1 10 '48067']\n",
            " [2 'M' 56 16 '70072']]\n",
            "[[        1      1193         5 978300760]\n",
            " [        1       661         3 978302109]]\n",
            "[[1 'Toy Story (1995)' \"Animation|Children's|Comedy\"]\n",
            " [2 'Jumanji (1995)' \"Adventure|Children's|Fantasy\"]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = pd.read_csv('u1.base', delimiter='\\t')\n",
        "test_set = pd.read_csv('u1.test', delimiter='\\t')\n",
        "training_set = np.array(training_set, dtype='int')\n",
        "test_set = np.array(test_set, dtype='int')"
      ],
      "metadata": {
        "id": "DLMSyiQlqybQ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "n_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
        "\n",
        "def convert(data):\n",
        "  matrix = []\n",
        "  for user_id in range(n_users):\n",
        "    movies_id = data[:, 1][data[:, 0] == user_id]\n",
        "    reviews = data[:, 2][data[:, 0] == user_id]\n",
        "\n",
        "    user_row = np.zeros(n_movies)\n",
        "    user_row[movies_id-1] = reviews\n",
        "    matrix.append(user_row)\n",
        "  return matrix\n",
        "\n",
        "training_set = torch.FloatTensor(np.array(convert(training_set)))\n",
        "test_set = torch.FloatTensor(np.array(convert(test_set)))"
      ],
      "metadata": {
        "id": "lJvlu65trnKD"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining model"
      ],
      "metadata": {
        "id": "mYuWZmkERERL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SAE(nn.Module):\n",
        "  def __init__(self, ):\n",
        "    super(SAE, self).__init__()\n",
        "    self.fc1 = nn.Linear(n_movies, 20)\n",
        "    self.fc2 = nn.Linear(20, 10)\n",
        "    self.fc3 = nn.Linear(10, 20)\n",
        "    self.fc4 = nn.Linear(20, n_movies)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.activation(self.fc1(x))\n",
        "    x = self.activation(self.fc2(x))\n",
        "    x = self.activation(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
      ],
      "metadata": {
        "id": "uCjwhOa0tvE1"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "_8rhA4kQRAIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 121\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0\n",
        "  s = 0\n",
        "\n",
        "  for user_id in range(n_users):\n",
        "    input = training_set[user_id].unsqueeze(0)\n",
        "    target = input.clone().detach()\n",
        "\n",
        "    if torch.sum(target > 0) > 0:\n",
        "      output = sae(input)\n",
        "      output[target == 0] = 0\n",
        "\n",
        "      loss = criterion(output, target)\n",
        "      mean_corrector = n_movies / float(torch.sum(target > 0) + 1e-10)\n",
        "      loss.backward()\n",
        "      total_loss += np.sqrt(loss.data*mean_corrector)\n",
        "      s += 1\n",
        "      optimizer.step()\n",
        "\n",
        "  print('epoch: ' + str(epoch) + ' loss: ' + str(total_loss/s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eYd9VztewmR7",
        "outputId": "ff02d133-f691-4d10-eeec-854b9b64b9f0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 loss: tensor(1.7718)\n",
            "epoch: 1 loss: tensor(1.0972)\n",
            "epoch: 2 loss: tensor(1.0538)\n",
            "epoch: 3 loss: tensor(1.0396)\n",
            "epoch: 4 loss: tensor(1.0324)\n",
            "epoch: 5 loss: tensor(1.0280)\n",
            "epoch: 6 loss: tensor(1.0255)\n",
            "epoch: 7 loss: tensor(1.0234)\n",
            "epoch: 8 loss: tensor(1.0223)\n",
            "epoch: 9 loss: tensor(1.0208)\n",
            "epoch: 10 loss: tensor(1.0201)\n",
            "epoch: 11 loss: tensor(1.0195)\n",
            "epoch: 12 loss: tensor(1.0188)\n",
            "epoch: 13 loss: tensor(1.0185)\n",
            "epoch: 14 loss: tensor(1.0183)\n",
            "epoch: 15 loss: tensor(1.0182)\n",
            "epoch: 16 loss: tensor(1.0179)\n",
            "epoch: 17 loss: tensor(1.0176)\n",
            "epoch: 18 loss: tensor(1.0174)\n",
            "epoch: 19 loss: tensor(1.0171)\n",
            "epoch: 20 loss: tensor(1.0175)\n",
            "epoch: 21 loss: tensor(1.0172)\n",
            "epoch: 22 loss: tensor(1.0171)\n",
            "epoch: 23 loss: tensor(1.0169)\n",
            "epoch: 24 loss: tensor(1.0168)\n",
            "epoch: 25 loss: tensor(1.0166)\n",
            "epoch: 26 loss: tensor(1.0166)\n",
            "epoch: 27 loss: tensor(1.0165)\n",
            "epoch: 28 loss: tensor(1.0166)\n",
            "epoch: 29 loss: tensor(1.0162)\n",
            "epoch: 30 loss: tensor(1.0160)\n",
            "epoch: 31 loss: tensor(1.0158)\n",
            "epoch: 32 loss: tensor(1.0148)\n",
            "epoch: 33 loss: tensor(1.0138)\n",
            "epoch: 34 loss: tensor(1.0127)\n",
            "epoch: 35 loss: tensor(1.0112)\n",
            "epoch: 36 loss: tensor(1.0093)\n",
            "epoch: 37 loss: tensor(1.0057)\n",
            "epoch: 38 loss: tensor(1.0034)\n",
            "epoch: 39 loss: tensor(1.0031)\n",
            "epoch: 40 loss: tensor(1.0010)\n",
            "epoch: 41 loss: tensor(0.9977)\n",
            "epoch: 42 loss: tensor(0.9948)\n",
            "epoch: 43 loss: tensor(0.9933)\n",
            "epoch: 44 loss: tensor(0.9923)\n",
            "epoch: 45 loss: tensor(0.9883)\n",
            "epoch: 46 loss: tensor(0.9865)\n",
            "epoch: 47 loss: tensor(0.9887)\n",
            "epoch: 48 loss: tensor(0.9863)\n",
            "epoch: 49 loss: tensor(0.9859)\n",
            "epoch: 50 loss: tensor(0.9834)\n",
            "epoch: 51 loss: tensor(0.9802)\n",
            "epoch: 52 loss: tensor(0.9762)\n",
            "epoch: 53 loss: tensor(0.9738)\n",
            "epoch: 54 loss: tensor(0.9717)\n",
            "epoch: 55 loss: tensor(0.9674)\n",
            "epoch: 56 loss: tensor(0.9691)\n",
            "epoch: 57 loss: tensor(0.9675)\n",
            "epoch: 58 loss: tensor(0.9649)\n",
            "epoch: 59 loss: tensor(0.9670)\n",
            "epoch: 60 loss: tensor(0.9616)\n",
            "epoch: 61 loss: tensor(0.9609)\n",
            "epoch: 62 loss: tensor(0.9608)\n",
            "epoch: 63 loss: tensor(0.9594)\n",
            "epoch: 64 loss: tensor(0.9561)\n",
            "epoch: 65 loss: tensor(0.9528)\n",
            "epoch: 66 loss: tensor(0.9560)\n",
            "epoch: 67 loss: tensor(0.9556)\n",
            "epoch: 68 loss: tensor(0.9540)\n",
            "epoch: 69 loss: tensor(0.9536)\n",
            "epoch: 70 loss: tensor(0.9527)\n",
            "epoch: 71 loss: tensor(0.9505)\n",
            "epoch: 72 loss: tensor(0.9502)\n",
            "epoch: 73 loss: tensor(0.9486)\n",
            "epoch: 74 loss: tensor(0.9502)\n",
            "epoch: 75 loss: tensor(0.9505)\n",
            "epoch: 76 loss: tensor(0.9517)\n",
            "epoch: 77 loss: tensor(0.9514)\n",
            "epoch: 78 loss: tensor(0.9580)\n",
            "epoch: 79 loss: tensor(0.9649)\n",
            "epoch: 80 loss: tensor(0.9597)\n",
            "epoch: 81 loss: tensor(0.9597)\n",
            "epoch: 82 loss: tensor(0.9600)\n",
            "epoch: 83 loss: tensor(0.9567)\n",
            "epoch: 84 loss: tensor(0.9541)\n",
            "epoch: 85 loss: tensor(0.9513)\n",
            "epoch: 86 loss: tensor(0.9502)\n",
            "epoch: 87 loss: tensor(0.9511)\n",
            "epoch: 88 loss: tensor(0.9476)\n",
            "epoch: 89 loss: tensor(0.9458)\n",
            "epoch: 90 loss: tensor(0.9444)\n",
            "epoch: 91 loss: tensor(0.9454)\n",
            "epoch: 92 loss: tensor(0.9439)\n",
            "epoch: 93 loss: tensor(0.9430)\n",
            "epoch: 94 loss: tensor(0.9430)\n",
            "epoch: 95 loss: tensor(0.9422)\n",
            "epoch: 96 loss: tensor(0.9393)\n",
            "epoch: 97 loss: tensor(0.9392)\n",
            "epoch: 98 loss: tensor(0.9408)\n",
            "epoch: 99 loss: tensor(0.9372)\n",
            "epoch: 100 loss: tensor(0.9353)\n",
            "epoch: 101 loss: tensor(0.9355)\n",
            "epoch: 102 loss: tensor(0.9360)\n",
            "epoch: 103 loss: tensor(0.9346)\n",
            "epoch: 104 loss: tensor(0.9337)\n",
            "epoch: 105 loss: tensor(0.9366)\n",
            "epoch: 106 loss: tensor(0.9341)\n",
            "epoch: 107 loss: tensor(0.9313)\n",
            "epoch: 108 loss: tensor(0.9303)\n",
            "epoch: 109 loss: tensor(0.9312)\n",
            "epoch: 110 loss: tensor(0.9328)\n",
            "epoch: 111 loss: tensor(0.9309)\n",
            "epoch: 112 loss: tensor(0.9295)\n",
            "epoch: 113 loss: tensor(0.9308)\n",
            "epoch: 114 loss: tensor(0.9292)\n",
            "epoch: 115 loss: tensor(0.9291)\n",
            "epoch: 116 loss: tensor(0.9315)\n",
            "epoch: 117 loss: tensor(0.9305)\n",
            "epoch: 118 loss: tensor(0.9287)\n",
            "epoch: 119 loss: tensor(0.9268)\n",
            "epoch: 120 loss: tensor(0.9265)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "0fcKL4YNQ7_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for user_id in range(n_users):\n",
        "    input = training_set[user_id].unsqueeze(0)\n",
        "    target = test_set[user_id].unsqueeze(0).detach()\n",
        "\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input)\n",
        "        output[target == 0] = 0\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = n_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
        "\n",
        "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
        "        s += 1.\n",
        "print('test loss: '+str(test_loss/s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YRzxQkU232ra",
        "outputId": "ebccacca-9d58-41db-8c01-c3b944ee5a63"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: tensor(0.9580)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code"
      ],
      "metadata": {
        "id": "qX1Em35y4AkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.parallel\n",
        "from torch.autograd import Variable\n",
        "\n",
        "users = pd.read_csv('users.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "movies = pd.read_csv('movies.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "reviews = pd.read_csv('ratings.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "\n",
        "training_set = pd.read_csv('u1.base', delimiter='\\t')\n",
        "test_set = pd.read_csv('u1.test', delimiter='\\t')\n",
        "training_set = np.array(training_set, dtype='int')\n",
        "test_set = np.array(test_set, dtype='int')\n",
        "\n",
        "n_users = max(max(training_set[:, 0]), max(test_set[:, 0]))\n",
        "n_movies = max(max(training_set[:, 1]), max(test_set[:, 1]))\n",
        "\n",
        "def convert(data):\n",
        "  matrix = []\n",
        "  for user_id in range(n_users):\n",
        "    movies_reviewed = data[:, 1][data[:, 0] == user_id]\n",
        "    grades = data[:, 2][data[:, 0] == user_id]\n",
        "\n",
        "    review = np.zeros(n_movies)\n",
        "    review[movies_reviewed-1] = grades\n",
        "    matrix.append(review)\n",
        "  return matrix\n",
        "\n",
        "training_set = torch.FloatTensor(convert(training_set))\n",
        "test_set = torch.FloatTensor(convert(test_set))\n",
        "\n",
        "class SAE(nn.Module):\n",
        "  def __init__(self, ):\n",
        "    super(SAE, self).__init__()\n",
        "    self.fc1 = nn.Linear(n_movies, 20)\n",
        "    self.fc2 = nn.Linear(20, 10)\n",
        "    self.fc3 = nn.Linear(10, 20)\n",
        "    self.fc4 = nn.Linear(20, n_movies)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.activation(self.fc1(x))\n",
        "    x = self.activation(self.fc2(x))\n",
        "    x = self.activation(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)\n",
        "\n",
        "n_epochs = 121\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0\n",
        "  s = 0\n",
        "\n",
        "  for user_id in range(n_users):\n",
        "    input = training_set[user_id].unsqueeze(0)\n",
        "    target = input.clone().detach()\n",
        "\n",
        "    if torch.sum(target > 0) > 0:\n",
        "      output = sae(input)\n",
        "      output[target == 0] = 0\n",
        "\n",
        "      loss = criterion(output, target)\n",
        "      mean_corrector = n_movies / float(torch.sum(target > 0) + 1e-10)\n",
        "      loss.backward()\n",
        "      total_loss += np.sqrt(loss.data*mean_corrector)\n",
        "      s += 1\n",
        "      optimizer.step()\n",
        "\n",
        "  print('epoch: ' + str(epoch) + ' loss: ' + str(total_loss/s))\n",
        "\n",
        "\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for user_id in range(n_users):\n",
        "    input = training_set[user_id].unsqueeze(0)\n",
        "    target = test_set[user_id].unsqueeze(0).detach()\n",
        "\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input)\n",
        "        output[target == 0] = 0\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = n_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
        "\n",
        "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
        "        s += 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qxHP26Sl4DK7",
        "outputId": "f9353dcf-1849-49f5-d336-83b5aff59a9b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 loss: tensor(1.7726)\n",
            "epoch: 1 loss: tensor(1.0973)\n",
            "epoch: 2 loss: tensor(1.0540)\n",
            "epoch: 3 loss: tensor(1.0398)\n",
            "epoch: 4 loss: tensor(1.0325)\n",
            "epoch: 5 loss: tensor(1.0284)\n",
            "epoch: 6 loss: tensor(1.0255)\n",
            "epoch: 7 loss: tensor(1.0233)\n",
            "epoch: 8 loss: tensor(1.0221)\n",
            "epoch: 9 loss: tensor(1.0209)\n",
            "epoch: 10 loss: tensor(1.0201)\n",
            "epoch: 11 loss: tensor(1.0196)\n",
            "epoch: 12 loss: tensor(1.0191)\n",
            "epoch: 13 loss: tensor(1.0186)\n",
            "epoch: 14 loss: tensor(1.0182)\n",
            "epoch: 15 loss: tensor(1.0181)\n",
            "epoch: 16 loss: tensor(1.0179)\n",
            "epoch: 17 loss: tensor(1.0174)\n",
            "epoch: 18 loss: tensor(1.0174)\n",
            "epoch: 19 loss: tensor(1.0173)\n",
            "epoch: 20 loss: tensor(1.0171)\n",
            "epoch: 21 loss: tensor(1.0172)\n",
            "epoch: 22 loss: tensor(1.0171)\n",
            "epoch: 23 loss: tensor(1.0168)\n",
            "epoch: 24 loss: tensor(1.0169)\n",
            "epoch: 25 loss: tensor(1.0169)\n",
            "epoch: 26 loss: tensor(1.0166)\n",
            "epoch: 27 loss: tensor(1.0164)\n",
            "epoch: 28 loss: tensor(1.0164)\n",
            "epoch: 29 loss: tensor(1.0160)\n",
            "epoch: 30 loss: tensor(1.0160)\n",
            "epoch: 31 loss: tensor(1.0146)\n",
            "epoch: 32 loss: tensor(1.0138)\n",
            "epoch: 33 loss: tensor(1.0126)\n",
            "epoch: 34 loss: tensor(1.0111)\n",
            "epoch: 35 loss: tensor(1.0083)\n",
            "epoch: 36 loss: tensor(1.0065)\n",
            "epoch: 37 loss: tensor(1.0046)\n",
            "epoch: 38 loss: tensor(1.0009)\n",
            "epoch: 39 loss: tensor(0.9963)\n",
            "epoch: 40 loss: tensor(0.9952)\n",
            "epoch: 41 loss: tensor(0.9930)\n",
            "epoch: 42 loss: tensor(0.9933)\n",
            "epoch: 43 loss: tensor(0.9904)\n",
            "epoch: 44 loss: tensor(0.9881)\n",
            "epoch: 45 loss: tensor(0.9854)\n",
            "epoch: 46 loss: tensor(0.9874)\n",
            "epoch: 47 loss: tensor(0.9861)\n",
            "epoch: 48 loss: tensor(0.9823)\n",
            "epoch: 49 loss: tensor(0.9820)\n",
            "epoch: 50 loss: tensor(0.9827)\n",
            "epoch: 51 loss: tensor(0.9789)\n",
            "epoch: 52 loss: tensor(0.9771)\n",
            "epoch: 53 loss: tensor(0.9748)\n",
            "epoch: 54 loss: tensor(0.9707)\n",
            "epoch: 55 loss: tensor(0.9722)\n",
            "epoch: 56 loss: tensor(0.9733)\n",
            "epoch: 57 loss: tensor(0.9697)\n",
            "epoch: 58 loss: tensor(0.9731)\n",
            "epoch: 59 loss: tensor(0.9677)\n",
            "epoch: 60 loss: tensor(0.9658)\n",
            "epoch: 61 loss: tensor(0.9623)\n",
            "epoch: 62 loss: tensor(0.9618)\n",
            "epoch: 63 loss: tensor(0.9613)\n",
            "epoch: 64 loss: tensor(0.9606)\n",
            "epoch: 65 loss: tensor(0.9563)\n",
            "epoch: 66 loss: tensor(0.9593)\n",
            "epoch: 67 loss: tensor(0.9576)\n",
            "epoch: 68 loss: tensor(0.9525)\n",
            "epoch: 69 loss: tensor(0.9483)\n",
            "epoch: 70 loss: tensor(0.9503)\n",
            "epoch: 71 loss: tensor(0.9512)\n",
            "epoch: 72 loss: tensor(0.9480)\n",
            "epoch: 73 loss: tensor(0.9488)\n",
            "epoch: 74 loss: tensor(0.9486)\n",
            "epoch: 75 loss: tensor(0.9458)\n",
            "epoch: 76 loss: tensor(0.9433)\n",
            "epoch: 77 loss: tensor(0.9443)\n",
            "epoch: 78 loss: tensor(0.9433)\n",
            "epoch: 79 loss: tensor(0.9411)\n",
            "epoch: 80 loss: tensor(0.9388)\n",
            "epoch: 81 loss: tensor(0.9387)\n",
            "epoch: 82 loss: tensor(0.9453)\n",
            "epoch: 83 loss: tensor(0.9416)\n",
            "epoch: 84 loss: tensor(0.9382)\n",
            "epoch: 85 loss: tensor(0.9356)\n",
            "epoch: 86 loss: tensor(0.9396)\n",
            "epoch: 87 loss: tensor(0.9374)\n",
            "epoch: 88 loss: tensor(0.9354)\n",
            "epoch: 89 loss: tensor(0.9338)\n",
            "epoch: 90 loss: tensor(0.9357)\n",
            "epoch: 91 loss: tensor(0.9364)\n",
            "epoch: 92 loss: tensor(0.9345)\n",
            "epoch: 93 loss: tensor(0.9327)\n",
            "epoch: 94 loss: tensor(0.9311)\n",
            "epoch: 95 loss: tensor(0.9339)\n",
            "epoch: 96 loss: tensor(0.9327)\n",
            "epoch: 97 loss: tensor(0.9302)\n",
            "epoch: 98 loss: tensor(0.9286)\n",
            "epoch: 99 loss: tensor(0.9309)\n",
            "epoch: 100 loss: tensor(0.9305)\n",
            "epoch: 101 loss: tensor(0.9285)\n",
            "epoch: 102 loss: tensor(0.9297)\n",
            "epoch: 103 loss: tensor(0.9299)\n",
            "epoch: 104 loss: tensor(0.9282)\n",
            "epoch: 105 loss: tensor(0.9266)\n",
            "epoch: 106 loss: tensor(0.9255)\n",
            "epoch: 107 loss: tensor(0.9246)\n",
            "epoch: 108 loss: tensor(0.9271)\n",
            "epoch: 109 loss: tensor(0.9301)\n",
            "epoch: 110 loss: tensor(0.9280)\n",
            "epoch: 111 loss: tensor(0.9252)\n",
            "epoch: 112 loss: tensor(0.9235)\n",
            "epoch: 113 loss: tensor(0.9239)\n",
            "epoch: 114 loss: tensor(0.9258)\n",
            "epoch: 115 loss: tensor(0.9244)\n",
            "epoch: 116 loss: tensor(0.9230)\n",
            "epoch: 117 loss: tensor(0.9218)\n",
            "epoch: 118 loss: tensor(0.9243)\n",
            "epoch: 119 loss: tensor(0.9243)\n",
            "epoch: 120 loss: tensor(0.9230)\n",
            "test loss: tensor(0.9516)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "users = pd.read_csv('users.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "movies = pd.read_csv('movies.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "reviews = pd.read_csv('ratings.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
        "\n",
        "training_set = pd.read_csv('u1.base', delimiter='\\t', header=None)\n",
        "test_set = pd.read_csv('u1.test', delimiter='\\t', header=None)\n",
        "\n",
        "training_set = np.array(training_set, dtype=int)\n",
        "test_set = np.array(test_set, dtype=int)\n",
        "\n",
        "n_users = max(max(training_set[:, 0]), max(test_set[:, 0])) + 1\n",
        "n_movies = max(max(training_set[:, 1]), max(test_set[:, 1])) + 1\n",
        "\n",
        "def convert(data):\n",
        "    matrix = np.zeros((n_users, n_movies), dtype=np.float32)\n",
        "    for user_id in range(n_users):\n",
        "\n",
        "        movies_reviewed = data[:, 1][data[:, 0] == user_id]\n",
        "        grades = data[:, 2][data[:, 0] == user_id]\n",
        "\n",
        "        if len(movies_reviewed) > 0:\n",
        "            matrix[user_id, movies_reviewed - 1] = grades\n",
        "\n",
        "    return matrix\n",
        "\n",
        "training_data = convert(training_set)\n",
        "test_data = convert(test_set)\n",
        "\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((training_data, training_data))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=n_users).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_data)).batch(batch_size)\n",
        "\n",
        "class SAE(Model):\n",
        "    def __init__(self):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = Dense(20, activation='sigmoid', kernel_regularizer=l2(0.5))\n",
        "        self.fc2 = Dense(10, activation='sigmoid', kernel_regularizer=l2(0.5))\n",
        "        self.fc3 = Dense(20, activation='sigmoid', kernel_regularizer=l2(0.5))\n",
        "        self.fc4 = Dense(n_movies, kernel_regularizer=l2(0.5))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "sae = SAE()\n",
        "\n",
        "sae.compile(optimizer=RMSprop(learning_rate=0.01),\n",
        "            loss=keras.losses.MeanSquaredError(),\n",
        "            metrics=['mse'])\n",
        "\n",
        "n_epochs = 121\n",
        "sae.fit(train_dataset, epochs=n_epochs, validation_data=test_dataset)\n",
        "\n",
        "test_loss = sae.evaluate(test_dataset)\n",
        "print(f\"test Loss: {test_loss[0]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QzRMJ0HBEeN_",
        "outputId": "e0741a2d-288c-4856-957e-9ac0adc6b34d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 1211.9124 - mse: 1.1160 - val_loss: 490.4383 - val_mse: 0.2509\n",
            "Epoch 2/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 395.2484 - mse: 0.7213 - val_loss: 178.7442 - val_mse: 0.1786\n",
            "Epoch 3/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 140.3985 - mse: 0.6587 - val_loss: 53.6859 - val_mse: 0.1786\n",
            "Epoch 4/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 40.1968 - mse: 0.6408 - val_loss: 11.3989 - val_mse: 0.1870\n",
            "Epoch 5/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3325 - mse: 0.6110 - val_loss: 1.9406 - val_mse: 0.1936\n",
            "Epoch 6/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.9414 - mse: 0.6130 - val_loss: 0.9872 - val_mse: 0.1978\n",
            "Epoch 7/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3793 - mse: 0.6045 - val_loss: 0.9747 - val_mse: 0.2044\n",
            "Epoch 8/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3733 - mse: 0.5938 - val_loss: 1.0180 - val_mse: 0.2070\n",
            "Epoch 9/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3739 - mse: 0.5564 - val_loss: 1.0541 - val_mse: 0.2142\n",
            "Epoch 10/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4403 - mse: 0.5984 - val_loss: 1.0559 - val_mse: 0.2156\n",
            "Epoch 11/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4323 - mse: 0.5914 - val_loss: 1.0631 - val_mse: 0.2201\n",
            "Epoch 12/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4387 - mse: 0.5964 - val_loss: 1.0637 - val_mse: 0.2192\n",
            "Epoch 13/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4406 - mse: 0.5975 - val_loss: 1.0634 - val_mse: 0.2222\n",
            "Epoch 14/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4311 - mse: 0.5888 - val_loss: 1.0650 - val_mse: 0.2224\n",
            "Epoch 15/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4414 - mse: 0.5983 - val_loss: 1.0680 - val_mse: 0.2248\n",
            "Epoch 16/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4421 - mse: 0.5991 - val_loss: 1.0664 - val_mse: 0.2238\n",
            "Epoch 17/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4243 - mse: 0.5812 - val_loss: 1.0692 - val_mse: 0.2260\n",
            "Epoch 18/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4402 - mse: 0.5969 - val_loss: 1.0664 - val_mse: 0.2241\n",
            "Epoch 19/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4353 - mse: 0.5923 - val_loss: 1.0699 - val_mse: 0.2267\n",
            "Epoch 20/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4330 - mse: 0.5900 - val_loss: 1.0686 - val_mse: 0.2246\n",
            "Epoch 21/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4342 - mse: 0.5907 - val_loss: 1.0710 - val_mse: 0.2270\n",
            "Epoch 22/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4477 - mse: 0.6044 - val_loss: 1.0679 - val_mse: 0.2244\n",
            "Epoch 23/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4254 - mse: 0.5826 - val_loss: 1.0679 - val_mse: 0.2262\n",
            "Epoch 24/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4294 - mse: 0.5868 - val_loss: 1.0666 - val_mse: 0.2250\n",
            "Epoch 25/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4354 - mse: 0.5928 - val_loss: 1.0680 - val_mse: 0.2262\n",
            "Epoch 26/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4183 - mse: 0.5753 - val_loss: 1.0672 - val_mse: 0.2249\n",
            "Epoch 27/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4617 - mse: 0.6186 - val_loss: 1.0684 - val_mse: 0.2259\n",
            "Epoch 28/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4267 - mse: 0.5834 - val_loss: 1.0670 - val_mse: 0.2246\n",
            "Epoch 29/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4727 - mse: 0.6292 - val_loss: 1.0698 - val_mse: 0.2260\n",
            "Epoch 30/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.4185 - mse: 0.5753 - val_loss: 1.0672 - val_mse: 0.2253\n",
            "Epoch 31/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4366 - mse: 0.5939 - val_loss: 1.0684 - val_mse: 0.2266\n",
            "Epoch 32/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4154 - mse: 0.5723 - val_loss: 1.0686 - val_mse: 0.2250\n",
            "Epoch 33/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.4193 - mse: 0.5757 - val_loss: 1.0695 - val_mse: 0.2265\n",
            "Epoch 34/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.4113 - mse: 0.5679 - val_loss: 1.0682 - val_mse: 0.2255\n",
            "Epoch 35/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4116 - mse: 0.5687 - val_loss: 1.0691 - val_mse: 0.2268\n",
            "Epoch 36/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4318 - mse: 0.5889 - val_loss: 1.0675 - val_mse: 0.2254\n",
            "Epoch 37/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.4203 - mse: 0.5775 - val_loss: 1.0689 - val_mse: 0.2266\n",
            "Epoch 38/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4330 - mse: 0.5902 - val_loss: 1.0677 - val_mse: 0.2250\n",
            "Epoch 39/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4335 - mse: 0.5903 - val_loss: 1.0693 - val_mse: 0.2264\n",
            "Epoch 40/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4170 - mse: 0.5739 - val_loss: 1.0682 - val_mse: 0.2250\n",
            "Epoch 41/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4662 - mse: 0.6233 - val_loss: 1.0687 - val_mse: 0.2259\n",
            "Epoch 42/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4147 - mse: 0.5717 - val_loss: 1.0685 - val_mse: 0.2249\n",
            "Epoch 43/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4250 - mse: 0.5819 - val_loss: 1.0711 - val_mse: 0.2268\n",
            "Epoch 44/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4395 - mse: 0.5965 - val_loss: 1.0673 - val_mse: 0.2244\n",
            "Epoch 45/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4439 - mse: 0.6010 - val_loss: 1.0695 - val_mse: 0.2263\n",
            "Epoch 46/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4427 - mse: 0.5997 - val_loss: 1.0676 - val_mse: 0.2242\n",
            "Epoch 47/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4439 - mse: 0.6010 - val_loss: 1.0687 - val_mse: 0.2259\n",
            "Epoch 48/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4242 - mse: 0.5814 - val_loss: 1.0666 - val_mse: 0.2247\n",
            "Epoch 49/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4250 - mse: 0.5823 - val_loss: 1.0701 - val_mse: 0.2266\n",
            "Epoch 50/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4283 - mse: 0.5849 - val_loss: 1.0675 - val_mse: 0.2250\n",
            "Epoch 51/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4290 - mse: 0.5862 - val_loss: 1.0691 - val_mse: 0.2263\n",
            "Epoch 52/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4558 - mse: 0.6126 - val_loss: 1.0687 - val_mse: 0.2243\n",
            "Epoch 53/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4253 - mse: 0.5819 - val_loss: 1.0701 - val_mse: 0.2261\n",
            "Epoch 54/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4403 - mse: 0.5971 - val_loss: 1.0687 - val_mse: 0.2246\n",
            "Epoch 55/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4275 - mse: 0.5841 - val_loss: 1.0691 - val_mse: 0.2258\n",
            "Epoch 56/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4280 - mse: 0.5851 - val_loss: 1.0680 - val_mse: 0.2249\n",
            "Epoch 57/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4416 - mse: 0.5986 - val_loss: 1.0672 - val_mse: 0.2255\n",
            "Epoch 58/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4302 - mse: 0.5875 - val_loss: 1.0670 - val_mse: 0.2249\n",
            "Epoch 59/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4446 - mse: 0.6017 - val_loss: 1.0699 - val_mse: 0.2261\n",
            "Epoch 60/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4532 - mse: 0.6099 - val_loss: 1.0669 - val_mse: 0.2248\n",
            "Epoch 61/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4246 - mse: 0.5818 - val_loss: 1.0683 - val_mse: 0.2258\n",
            "Epoch 62/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4305 - mse: 0.5875 - val_loss: 1.0678 - val_mse: 0.2250\n",
            "Epoch 63/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4414 - mse: 0.5982 - val_loss: 1.0701 - val_mse: 0.2260\n",
            "Epoch 64/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4408 - mse: 0.5973 - val_loss: 1.0680 - val_mse: 0.2245\n",
            "Epoch 65/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4556 - mse: 0.6123 - val_loss: 1.0698 - val_mse: 0.2260\n",
            "Epoch 66/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4213 - mse: 0.5780 - val_loss: 1.0678 - val_mse: 0.2247\n",
            "Epoch 67/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4366 - mse: 0.5936 - val_loss: 1.0686 - val_mse: 0.2258\n",
            "Epoch 68/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4313 - mse: 0.5884 - val_loss: 1.0676 - val_mse: 0.2247\n",
            "Epoch 69/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4746 - mse: 0.6315 - val_loss: 1.0680 - val_mse: 0.2250\n",
            "Epoch 70/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4338 - mse: 0.5909 - val_loss: 1.0667 - val_mse: 0.2242\n",
            "Epoch 71/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4360 - mse: 0.5930 - val_loss: 1.0692 - val_mse: 0.2254\n",
            "Epoch 72/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4365 - mse: 0.5933 - val_loss: 1.0679 - val_mse: 0.2243\n",
            "Epoch 73/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4319 - mse: 0.5886 - val_loss: 1.0694 - val_mse: 0.2260\n",
            "Epoch 74/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4251 - mse: 0.5821 - val_loss: 1.0678 - val_mse: 0.2247\n",
            "Epoch 75/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4394 - mse: 0.5967 - val_loss: 1.0686 - val_mse: 0.2255\n",
            "Epoch 76/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4461 - mse: 0.6030 - val_loss: 1.0678 - val_mse: 0.2243\n",
            "Epoch 77/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4365 - mse: 0.5933 - val_loss: 1.0685 - val_mse: 0.2254\n",
            "Epoch 78/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4475 - mse: 0.6044 - val_loss: 1.0674 - val_mse: 0.2246\n",
            "Epoch 79/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4435 - mse: 0.6004 - val_loss: 1.0694 - val_mse: 0.2258\n",
            "Epoch 80/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4279 - mse: 0.5845 - val_loss: 1.0685 - val_mse: 0.2252\n",
            "Epoch 81/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4373 - mse: 0.5940 - val_loss: 1.0685 - val_mse: 0.2257\n",
            "Epoch 82/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4193 - mse: 0.5762 - val_loss: 1.0685 - val_mse: 0.2248\n",
            "Epoch 83/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4512 - mse: 0.6079 - val_loss: 1.0686 - val_mse: 0.2258\n",
            "Epoch 84/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.4253 - mse: 0.5824 - val_loss: 1.0675 - val_mse: 0.2247\n",
            "Epoch 85/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4384 - mse: 0.5955 - val_loss: 1.0684 - val_mse: 0.2257\n",
            "Epoch 86/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4491 - mse: 0.6062 - val_loss: 1.0677 - val_mse: 0.2247\n",
            "Epoch 87/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4609 - mse: 0.6178 - val_loss: 1.0686 - val_mse: 0.2254\n",
            "Epoch 88/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3858 - mse: 0.5426 - val_loss: 1.0684 - val_mse: 0.2250\n",
            "Epoch 89/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4212 - mse: 0.5781 - val_loss: 1.0698 - val_mse: 0.2261\n",
            "Epoch 90/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4299 - mse: 0.5869 - val_loss: 1.0690 - val_mse: 0.2252\n",
            "Epoch 91/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4427 - mse: 0.5998 - val_loss: 1.0700 - val_mse: 0.2262\n",
            "Epoch 92/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4405 - mse: 0.5972 - val_loss: 1.0681 - val_mse: 0.2250\n",
            "Epoch 93/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4237 - mse: 0.5808 - val_loss: 1.0691 - val_mse: 0.2261\n",
            "Epoch 94/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4358 - mse: 0.5927 - val_loss: 1.0685 - val_mse: 0.2249\n",
            "Epoch 95/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4347 - mse: 0.5915 - val_loss: 1.0686 - val_mse: 0.2256\n",
            "Epoch 96/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4606 - mse: 0.6177 - val_loss: 1.0668 - val_mse: 0.2242\n",
            "Epoch 97/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4132 - mse: 0.5703 - val_loss: 1.0692 - val_mse: 0.2259\n",
            "Epoch 98/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4391 - mse: 0.5959 - val_loss: 1.0675 - val_mse: 0.2247\n",
            "Epoch 99/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4452 - mse: 0.6022 - val_loss: 1.0685 - val_mse: 0.2258\n",
            "Epoch 100/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4455 - mse: 0.6027 - val_loss: 1.0676 - val_mse: 0.2249\n",
            "Epoch 101/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.4116 - mse: 0.5687 - val_loss: 1.0694 - val_mse: 0.2262\n",
            "Epoch 102/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4111 - mse: 0.5678 - val_loss: 1.0689 - val_mse: 0.2253\n",
            "Epoch 103/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4410 - mse: 0.5978 - val_loss: 1.0681 - val_mse: 0.2257\n",
            "Epoch 104/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4409 - mse: 0.5980 - val_loss: 1.0673 - val_mse: 0.2245\n",
            "Epoch 105/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4327 - mse: 0.5896 - val_loss: 1.0690 - val_mse: 0.2258\n",
            "Epoch 106/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.4435 - mse: 0.6002 - val_loss: 1.0683 - val_mse: 0.2248\n",
            "Epoch 107/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4202 - mse: 0.5770 - val_loss: 1.0693 - val_mse: 0.2260\n",
            "Epoch 108/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.4284 - mse: 0.5853 - val_loss: 1.0682 - val_mse: 0.2250\n",
            "Epoch 109/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4283 - mse: 0.5854 - val_loss: 1.0694 - val_mse: 0.2259\n",
            "Epoch 110/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4376 - mse: 0.5944 - val_loss: 1.0678 - val_mse: 0.2253\n",
            "Epoch 111/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4449 - mse: 0.6021 - val_loss: 1.0686 - val_mse: 0.2257\n",
            "Epoch 112/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4336 - mse: 0.5909 - val_loss: 1.0676 - val_mse: 0.2250\n",
            "Epoch 113/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4256 - mse: 0.5828 - val_loss: 1.0685 - val_mse: 0.2257\n",
            "Epoch 114/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4295 - mse: 0.5863 - val_loss: 1.0683 - val_mse: 0.2247\n",
            "Epoch 115/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4508 - mse: 0.6073 - val_loss: 1.0688 - val_mse: 0.2253\n",
            "Epoch 116/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4573 - mse: 0.6140 - val_loss: 1.0677 - val_mse: 0.2243\n",
            "Epoch 117/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4401 - mse: 0.5967 - val_loss: 1.0693 - val_mse: 0.2257\n",
            "Epoch 118/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4449 - mse: 0.6018 - val_loss: 1.0676 - val_mse: 0.2245\n",
            "Epoch 119/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4210 - mse: 0.5780 - val_loss: 1.0685 - val_mse: 0.2258\n",
            "Epoch 120/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4698 - mse: 0.6269 - val_loss: 1.0678 - val_mse: 0.2244\n",
            "Epoch 121/121\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4277 - mse: 0.5845 - val_loss: 1.0691 - val_mse: 0.2255\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1559 - mse: 0.3123\n",
            "Test Loss: 1.0691\n"
          ]
        }
      ]
    }
  ]
}